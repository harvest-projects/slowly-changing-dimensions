{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee13555-95ec-475d-83d1-983ebefe6d90",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions - Type 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f540a729-1112-412b-a081-44e4631339d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d0715f-b512-4088-90e9-482f855eb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68521c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession.builder.appName('scd1').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a69a0-bbff-4af8-949c-11a18db1c3f2",
   "metadata": {},
   "source": [
    "## 1. Preperation steps\n",
    "In the following cells we will perform the following steps:\n",
    "1. Read-in our target dataframe\n",
    "2. Add our technical columns to this dataframe\n",
    "3. Save the target dataframe as our 'source' dataframe (initial load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23925c-a067-432f-b955-35743c4c9778",
   "metadata": {},
   "source": [
    "### Read-in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5dc588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|INDEX|    CUSTOMER_ID|FIRST_NAME|LAST_NAME|             COMPANY|             CITY|           COUNTRY|             PHONE_1|         PHONE_2|               EMAIL|SUBSCRIPTION_DATE|             WEBSITE|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|    1|DD37Cf93aecA6Dc|   Sheryll|   Baxter|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    1|xxxxxxxxxxxxxxx|Skaarrrrrt|    Baked|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    2|1Ef7b82A4CAAD10|   Preston|   Lozano|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    2|yyyyyyyyyyyyyyy|  Preston2|  Lozano2|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    4|5Cef8BFA16c5e3c|     Linda|    Olsen|Dominguez, Mcmill...|       Bensonview|Dominican Republic|001-808-617-6467x...| +1-813-324-8756|stanleyblackwell@...|       2020-06-02|http://www.good-l...|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = T.StructType([\n",
    "    T.StructField('INDEX', T.IntegerType(), True), \n",
    "    T.StructField('CUSTOMER_ID', T.StringType(), True), \n",
    "    T.StructField('FIRST_NAME', T.StringType(), True), \n",
    "    T.StructField('LAST_NAME', T.StringType(), True), \n",
    "    T.StructField('COMPANY', T.StringType(), True), \n",
    "    T.StructField('CITY', T.StringType(), True), \n",
    "    T.StructField('COUNTRY', T.StringType(), True), \n",
    "    T.StructField('PHONE_1', T.StringType(), True), \n",
    "    T.StructField('PHONE_2', T.StringType(), True), \n",
    "    T.StructField('EMAIL', T.StringType(), True), \n",
    "    T.StructField('SUBSCRIPTION_DATE', T.DateType(), True), \n",
    "    T.StructField('WEBSITE', T.StringType(), True)\n",
    "])\n",
    "\n",
    "# Read-in dataframe\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('header', True)\n",
    "    .option('schema', schema)\n",
    "    .csv('scd1_data/source.csv')\n",
    ")\n",
    "\n",
    "# Show dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b1671-70d3-470d-b4b3-c602cdcad986",
   "metadata": {},
   "source": [
    "### Saving dataframe as 'output'\n",
    "PySpark has a particular way of saving parquet, delta, and csv files.\n",
    "Because of this, we need to create a helper function, so that our output is saved as a single csv file.\n",
    "Do not worry to much about understanding this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5dd5a679-9cf5-46df-85d8-1800f62861b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv(df: DataFrame, file_path: str):\n",
    "    tmp_folder = file_path + 'tmp'\n",
    "    \n",
    "    # Save DataFrame to a temporary folder\n",
    "    (\n",
    "        df\n",
    "        .coalesce(1)  # Ensure a single partition\n",
    "        .write\n",
    "        .mode('overwrite')\n",
    "        .format('csv')\n",
    "        .option('header', True)\n",
    "        .save(tmp_folder)\n",
    "    )\n",
    "    \n",
    "    # Find the single partition file\n",
    "    for file_name in os.listdir(tmp_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            tmp_file_path = os.path.join(tmp_folder, file_name)\n",
    "            break\n",
    "    \n",
    "    # Move and rename the file to the final destination\n",
    "    shutil.move(tmp_file_path, file_path)\n",
    "    \n",
    "    # Remove the temporary folder\n",
    "    shutil.rmtree(tmp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fad5cdb1-c1d6-4f74-b800-afe3eda4611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(df=df, file_path='scd1_data/target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76894016-84b7-4d0d-854c-60dd8ecc97cc",
   "metadata": {},
   "source": [
    "## 2. Starting the SCD1 Proces\n",
    "Now, we will begin with the implementation of the Slowly Changing Dimensions type 1. We will be implementing the following steps:\n",
    "1. Change the target dataframe by adding or editing some rows.\n",
    "2. Read-in the target and source dataframe.\n",
    "3. Select the rows in source dataframe that are new.\n",
    "4. Select the rows in source dataframe that have been deleted.\n",
    "5. Select the rows in source dataframe that are updated.\n",
    "6. Insert, update, and/or delete the selected rows in the source dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1275ee-88a1-4bb2-89e2-1feeef75187f",
   "metadata": {},
   "source": [
    "### Step 1: Change the target dataframe by adding or editing some rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed422402-0e1c-49d3-93ab-62ff7e449b84",
   "metadata": {},
   "source": [
    "* Make some alterations to the source data.\n",
    "* You can find this file under notebooks/scd1_data/source.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69caf3a8-89d0-4d98-a02c-4185a8fc1655",
   "metadata": {},
   "source": [
    "### Step 2: Read-in the target and source dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6559d2d3-7ea1-4173-815e-f6d4308b10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in the source dataframe\n",
    "source_schema = T.StructType([\n",
    "    T.StructField('INDEX', T.IntegerType(), True), \n",
    "    T.StructField('CUSTOMER_ID', T.StringType(), True), \n",
    "    T.StructField('FIRST_NAME', T.StringType(), True), \n",
    "    T.StructField('LAST_NAME', T.StringType(), True), \n",
    "    T.StructField('COMPANY', T.StringType(), True), \n",
    "    T.StructField('CITY', T.StringType(), True), \n",
    "    T.StructField('COUNTRY', T.StringType(), True), \n",
    "    T.StructField('PHONE_1', T.StringType(), True), \n",
    "    T.StructField('PHONE_2', T.StringType(), True), \n",
    "    T.StructField('EMAIL', T.StringType(), True), \n",
    "    T.StructField('SUBSCRIPTION_DATE', T.DateType(), True), \n",
    "    T.StructField('WEBSITE', T.StringType(), True)\n",
    "])\n",
    "source_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('header', True)\n",
    "    .option('schema', schema)\n",
    "    .csv('scd1_data/source.csv')\n",
    ")\n",
    "\n",
    "# Read-in the target dataframe\n",
    "target_schema = T.StructType([\n",
    "    T.StructField('INDEX', T.IntegerType(), True), \n",
    "    T.StructField('CUSTOMER_ID', T.StringType(), True), \n",
    "    T.StructField('FIRST_NAME', T.StringType(), True), \n",
    "    T.StructField('LAST_NAME', T.StringType(), True), \n",
    "    T.StructField('COMPANY', T.StringType(), True), \n",
    "    T.StructField('CITY', T.StringType(), True), \n",
    "    T.StructField('COUNTRY', T.StringType(), True), \n",
    "    T.StructField('PHONE_1', T.StringType(), True), \n",
    "    T.StructField('PHONE_2', T.StringType(), True), \n",
    "    T.StructField('EMAIL', T.StringType(), True), \n",
    "    T.StructField('SUBSCRIPTION_DATE', T.DateType(), True), \n",
    "    T.StructField('WEBSITE', T.StringType(), True),\n",
    "])\n",
    "target_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .option('header', True)\n",
    "    .option('schema', schema)\n",
    "    .csv('scd1_data/target.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "afcb58f5-78ed-4896-81b7-3205a9f0aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "|INDEX|    CUSTOMER_ID|FIRST_NAME|LAST_NAME|        COMPANY|             CITY| COUNTRY|     PHONE_1|         PHONE_2|               EMAIL|SUBSCRIPTION_DATE|             WEBSITE|\n",
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "|    1|DD37Cf93aecA6Dc|   Sheryll|   Baxter|Rasmussen Group|     East Leonard|   Chile|229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    1|xxxxxxxxxxxxxxx|Skaarrrrrt|    Baked|Rasmussen Group|     East Leonard|   Chile|229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    2|1Ef7b82A4CAAD10|   Preston|   Lozano|    Vega-Gentry|East Jimmychester|Djibouti|  5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    2|yyyyyyyyyyyyyyy|  Preston2|  Lozano2|    Vega-Gentry|East Jimmychester|Djibouti|  5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    2|             aa|  Preston2|  Lozano2|    Vega-Gentry|East Jimmychester|Djibouti|  5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "+-----+---------------+----------+---------+---------------+-----------------+--------+------------+----------------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "66294b22-58ff-424d-8c7e-f9b91e0ff679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|INDEX|    CUSTOMER_ID|FIRST_NAME|LAST_NAME|             COMPANY|             CITY|           COUNTRY|             PHONE_1|         PHONE_2|               EMAIL|SUBSCRIPTION_DATE|             WEBSITE|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|    1|DD37Cf93aecA6Dc|   Sheryll|   Baxter|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    1|xxxxxxxxxxxxxxx|Skaarrrrrt|    Baked|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    2|1Ef7b82A4CAAD10|   Preston|   Lozano|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    2|yyyyyyyyyyyyyyy|  Preston2|  Lozano2|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    4|5Cef8BFA16c5e3c|     Linda|    Olsen|Dominguez, Mcmill...|       Bensonview|Dominican Republic|001-808-617-6467x...| +1-813-324-8756|stanleyblackwell@...|       2020-06-02|http://www.good-l...|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced99af-d008-484b-bc2a-3140e910fcf1",
   "metadata": {},
   "source": [
    "### Step 3: Select the rows in source dataframe that are new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "647ea3e5-62c3-40aa-bb5a-7be5bc460292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inserts(source_df: DataFrame, target_df: DataFrame):\n",
    "    # Find rows in source_df that are not present in target_df\n",
    "    insert_df = source_df.join(target_df, on='CUSTOMER_ID', how='leftanti')\n",
    "    \n",
    "    return insert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "906eb4f9-61b2-4b02-8996-c84d3469ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_df = get_inserts(source_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "578535b7-8c51-43c0-be98-2d03813dc25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+----------+---------+-----------+-----------------+--------+----------+----------------+---------------+-----------------+--------------------+\n",
      "|CUSTOMER_ID|INDEX|FIRST_NAME|LAST_NAME|    COMPANY|             CITY| COUNTRY|   PHONE_1|         PHONE_2|          EMAIL|SUBSCRIPTION_DATE|             WEBSITE|\n",
      "+-----------+-----+----------+---------+-----------+-----------------+--------+----------+----------------+---------------+-----------------+--------------------+\n",
      "|         aa|    2|  Preston2|  Lozano2|Vega-Gentry|East Jimmychester|Djibouti|5153435776|686-620-1820x944|vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|         bb|    2|  Preston2|  Lozano2|Vega-Gentry|East Jimmychester|Djibouti|5153435776|686-620-1820x944|vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|         cc|    2|  Preston2|  Lozano2|Vega-Gentry|East Jimmychester|Djibouti|5153435776|686-620-1820x944|vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|         dd|    2|  Preston2|  Lozano2|Vega-Gentry|East Jimmychester|Djibouti|5153435776|686-620-1820x944|vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "+-----------+-----+----------+---------+-----------+-----------------+--------+----------+----------------+---------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21266c-f372-4c4a-a09f-410c3ea5582e",
   "metadata": {},
   "source": [
    "### Step 4: Select the rows in source dataframe that have been deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "508b69dd-c98f-47f1-a190-e3f6f4a134f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deletes(source_df: DataFrame, target_df: DataFrame):\n",
    "    # Find rows in target_df that are not present in source_df\n",
    "    delete_df = target_df.join(source_df, on='CUSTOMER_ID', how='leftanti')\n",
    "    \n",
    "    return delete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "20f41186-e07a-4fe5-bdcb-1b47f5d6649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_df = get_deletes(source_df, target_df).select(*source_df.columns) # extra select to keep column order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e74b9286-f9a4-4813-941c-f0cc330c451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "|INDEX|CUSTOMER_ID|FIRST_NAME|LAST_NAME|COMPANY|CITY|COUNTRY|PHONE_1|PHONE_2|EMAIL|SUBSCRIPTION_DATE|WEBSITE|\n",
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delete_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac789a5-9803-420a-af1f-0ae4a7fca15b",
   "metadata": {},
   "source": [
    "### Step 5: Select the rows in source dataframe that are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a1fa1c1b-61ae-4b87-8801-afbb676ce61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hash_column(df: DataFrame, columns: list, hash_column_name: str = 'CTC_HASH') -> DataFrame:\n",
    "    # Add a hash column to the DataFrame based on the specified columns.\n",
    "    return df.withColumn(hash_column_name, F.sha2(F.concat_ws('||', *columns), 256))\n",
    "\n",
    "def get_updates(source_df: DataFrame, target_df: DataFrame, ctc_cols: list):\n",
    "    # Add hash columns based on the specified columns\n",
    "    source_df_hash = add_hash_column(source_df, ctc_cols)\n",
    "    target_df_hash = add_hash_column(target_df, ctc_cols)\n",
    "    \n",
    "    # Find corresponding rows between source_df and target_df\n",
    "    overlap_df = source_df_hash.alias('src').join(target_df_hash.alias('tgt'), on='CUSTOMER_ID', how='inner')\n",
    "    \n",
    "    # Apply filter to get rows where hash values are different\n",
    "    update_df = (\n",
    "        overlap_df\n",
    "        .filter(F.col('src.CTC_HASH') != F.col('tgt.CTC_HASH'))\n",
    "        .select('src.*')\n",
    "        .drop('CTC_HASH')\n",
    "    )\n",
    "    \n",
    "    return update_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2e8c2b25-5e2b-47e5-9ec5-8bb9119e0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cols = ['COSTUMER_ID']\n",
    "tech_cols = []    # empty now, but this will be filled in other SCD processes\n",
    "ctc_cols = [col for col in source_df.columns if col not in key_cols + tech_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "79128b73-eb73-4550-bf36-b8c1e2d5c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df = get_updates(source_df, target_df, ctc_cols).select(*source_df.columns) # extra select to keep column order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c82ef636-8df2-49a1-a7f3-01cf2fe95f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "|INDEX|CUSTOMER_ID|FIRST_NAME|LAST_NAME|COMPANY|CITY|COUNTRY|PHONE_1|PHONE_2|EMAIL|SUBSCRIPTION_DATE|WEBSITE|\n",
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "+-----+-----------+----------+---------+-------+----+-------+-------+-------+-----+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "update_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e019f59-bba7-468e-8a32-9ad35dbcbb7f",
   "metadata": {},
   "source": [
    "### Step 6: Insert, update, and/or delete the selected rows in the source dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "91a1c41d-85f4-4c3d-9c6f-652fb0aca798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the target DataFrame\n",
    "def update_target_df(target_df: DataFrame, insert_df: DataFrame, delete_df: DataFrame, update_df: DataFrame) -> DataFrame:\n",
    "    # Step 1: Delete rows in target_df that are present in delete_df\n",
    "    target_df = target_df.join(delete_df, on='CUSTOMER_ID', how='leftanti')\n",
    "    \n",
    "    # Step 2: Insert new rows from insert_df to target_df\n",
    "    target_df = target_df.union(insert_df)\n",
    "    \n",
    "    # Step 3: Update existing rows in target_df with rows from update_df\n",
    "    if not update_df.rdd.isEmpty():\n",
    "        # Remove rows from target_df that need to be updated\n",
    "        target_df = target_df.join(update_df.select('CUSTOMER_ID'), on='CUSTOMER_ID', how='leftanti')\n",
    "        # Add the updated rows\n",
    "        target_df = target_df.union(update_df.select(target_df.columns))\n",
    "    \n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a600a957-f28d-41d1-a971-6d3700f747b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = update_target_df(\n",
    "    target_df=target_df,\n",
    "    insert_df=insert_df,\n",
    "    delete_df=delete_df,\n",
    "    update_df=update_df\n",
    ").select(*[field.name for field in target_schema.fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "80603395-7bc7-4f70-ab6e-9e54c75e732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|INDEX|    CUSTOMER_ID|FIRST_NAME|LAST_NAME|             COMPANY|             CITY|           COUNTRY|             PHONE_1|         PHONE_2|               EMAIL|SUBSCRIPTION_DATE|             WEBSITE|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "|    1|DD37Cf93aecA6Dc|   Sheryll|   Baxter|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    1|xxxxxxxxxxxxxxx|Skaarrrrrt|    Baked|     Rasmussen Group|     East Leonard|             Chile|        229.077.5154|397.884.0519x718|zunigavanessa@smi...|       2020-08-24|http://www.stephe...|\n",
      "|    2|1Ef7b82A4CAAD10|   Preston|   Lozano|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    2|yyyyyyyyyyyyyyy|  Preston2|  Lozano2|         Vega-Gentry|East Jimmychester|          Djibouti|          5153435776|686-620-1820x944|     vmata@colon.com|       2021-04-23|http://www.hobbs....|\n",
      "|    4|5Cef8BFA16c5e3c|     Linda|    Olsen|Dominguez, Mcmill...|       Bensonview|Dominican Republic|001-808-617-6467x...| +1-813-324-8756|stanleyblackwell@...|       2020-06-02|http://www.good-l...|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+------------------+--------------------+----------------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4345e808-7acd-47dc-b6a0-5e1619d7e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_csv(df=target_df, file_path='scd1_data/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578b740-1c04-4415-8d7f-616ffd4144c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd2_create_staging_table(df_input: DataFrame, target_table: str, key_cols: List[str], ctc_cols: List[str], dts_from: str, dts_to: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares a staging DataFrame for a Slowly Changing Dimension Type 2 (SCD2) operation in a data warehouse\n",
    "    by identifying rows in the input DataFrame that need to be inserted or staated in the target table.\n",
    "\n",
    "    Parameters:\n",
    "    - df_input (DataFrame): The source DataFrame containing new data to be merged into the target table.\n",
    "    - target_table (str): The name of the target table in the data warehouse where updates and inserts will be applied.\n",
    "    - key_cols (List[str]): A list of column names that uniquely identify a row in the target table, used for matching rows between the input DataFrame and the target table.\n",
    "    - ctc_cols (List[str]): A list of column names used for change tracking. Rows with changes in any of these columns will be marked for update.\n",
    "    - dts_from (str): Start date/time column name in target table (e.g. 'LOAD_FROM_DTS').\n",
    "    - dts_to (str): Start date/time column name in target table (e.g. 'LOAD_UNTIL_DTS').\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A Spark DataFrame representing the staging area for SCD2 operations. \n",
    "    \"\"\"\n",
    "    # Create hash for input dataframe\n",
    "    if not \"BK_HASH\" in df_input.columns:\n",
    "        df_input = create_hash_key(df=df_input, cols=key_cols, hash_col_name=\"BK_HASH\")\n",
    "    if not \"CTC_HASH\" in df_input.columns:\n",
    "        df_input = create_hash_key(df=df_input, cols=ctc_cols, hash_col_name=\"CTC_HASH\")\n",
    "    df_input.cache()\n",
    "\n",
    "    # Create hash in target table\n",
    "    target_columns = spark.table(target_table).columns  # Accesses table's metadata\n",
    "    if not \"BK_HASH\" in target_columns:\n",
    "        create_hash_key_sql(table_name=target_table, cols=key_cols, hash_col_name=\"BK_HASH\")\n",
    "    if not \"CTC_HASH\" in target_columns:\n",
    "        create_hash_key_sql(table_name=target_table, cols=ctc_cols, hash_col_name=\"CTC_HASH\")\n",
    "\n",
    "    # Create view for df_input\n",
    "    input_view = create_temp_view(df_input)\n",
    "\n",
    "    # Do not select certain columns if they are already in df\n",
    "    cols_not_select = [dts_to, \"isCurrent\", \"INSERTUPD\"]\n",
    "\n",
    "    # Step 1: get all the insert rows.\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            {get_joined_cols(df=df_input, cols_not_select=cols_not_select, col_prefix=\"a\")}\n",
    "            ,CAST('{HI_DTS}' AS TIMESTAMP) AS {dts_to}  -- HI_DTS is defined as general constant\n",
    "            ,'Y' as isCurrent\n",
    "            ,'ins' as INSERTUPD\n",
    "        FROM {input_view} a\n",
    "        LEFT JOIN {target_table} b\n",
    "            ON a.BK_HASH = b.BK_HASH        -- Join on key columns\n",
    "            AND b.isCurrent = 'Y'\n",
    "        WHERE a.CTC_HASH <> b.CTC_HASH      -- 1. Row exists in target with different values\n",
    "            OR b.CTC_HASH IS NULL           -- 2. No row exists in target\n",
    "    \"\"\"\n",
    "    ins_df = spark.sql(query)\n",
    "\n",
    "    # Step 2: get all the update rows\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            a.*\n",
    "            ,'upd' AS INSERTUPD\n",
    "        FROM {input_view} a\n",
    "        LEFT JOIN {target_table} b\n",
    "            ON a.BK_HASH = b.BK_HASH        -- Join on key columns\n",
    "            AND b.isCurrent = 'Y'\n",
    "         WHERE a.CTC_HASH <> b.CTC_HASH     -- Row exists in target with different values\n",
    "    \"\"\"\n",
    "    upd_df = spark.sql(query)\n",
    "\n",
    "    # Step 3: union the insert and update rows into staging table\n",
    "    df_staging = union_all([ins_df, upd_df])\n",
    "\n",
    "    # Step 4: Add current datetime as LOAD_FROM_DTS\n",
    "    curr_datetime = get_current_datetime()\n",
    "    df_staging = df_staging.withColumn(dts_from, F.lit(curr_datetime))\n",
    "\n",
    "    # Unpersists cache\n",
    "    df_input.unpersist()\n",
    "    return df_staging\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def scd2_update_rows(df_updates: DataFrame, target_table: str, key_cols:List[str], dts_from: str, dts_to: str) -> None:\n",
    "    \"\"\"\n",
    "    Updates target table rows for SCD2, marking existing records as historical and updating their end dates \n",
    "    to the start dates of new staging DataFrame records.\n",
    "\n",
    "    Parameters:\n",
    "    - df_updates (DataFrame): DataFrame with update records.\n",
    "    - target_table (str): Name of the target table for updates.\n",
    "    - key_cols (List[str]): Unique key columns for record matching.\n",
    "    - dts_from (str): Start date/time column name in target table (e.g. 'LOAD_FROM_DTS').\n",
    "    - dts_to (str): End date/time column name for updated records (e.g. 'LOAD_UNTIL_DTS').\n",
    "\n",
    "    Returns: None. Directly updates the target table.\n",
    "    \"\"\"\n",
    "    # Load in the DeltaTable\n",
    "    deltaTable = DeltaTable.forName(spark, f\"default.{target_table}\")\n",
    "\n",
    "    # Define the matching condition\n",
    "    condition = \"t.BK_HASH = s.BK_HASH\"\n",
    "\n",
    "    # Perform merge (upsert) operation\n",
    "    deltaTable.alias(\"t\") \\\n",
    "        .merge(df_updates.alias(\"s\"), condition) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"isCurrent\": \"'N'\",\n",
    "            f\"{dts_to}\": f\"s.{dts_from}\"\n",
    "        }) \\\n",
    "        .execute()\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "  \n",
    "  def scd2_insert_rows(df_inserts: DataFrame, target_table: str) -> None:\n",
    "    \"\"\"\n",
    "    Inserts new SCD2 records from a staging DataFrame into the target table, identified by an 'ins' flag.\n",
    "\n",
    "    Parameters:\n",
    "    - df_inserts (DataFrame): Staging DataFrame with new/updated records.\n",
    "    - target_table (str): Target table name for record insertion.\n",
    "\n",
    "    Returns: None. Directly inserts new records into the target table.\n",
    "    \"\"\"\n",
    "    # Get columns of target table\n",
    "    target_columns = spark.table(target_table).columns  # Accesses table's metadata\n",
    "\n",
    "    # Insert rows in the target table\n",
    "    (\n",
    "        df_inserts\n",
    "        .select(*target_columns)\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('append')\n",
    "        .saveAsTable(target_table)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d757770-b0bd-403b-b778-4ffdb20abddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_columns(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .withColumn('VALID_FROM', F.current_timestamp().cast(T.TimestampType()))\n",
    "        .withColumn('VALID_TO', F.lit('9999-12-31 23:59:59').cast(T.TimestampType()))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
